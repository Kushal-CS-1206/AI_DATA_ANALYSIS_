{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in Large-scale ML Pipelines:\n",
    "\n",
    "**Task 1**: Impute with Mean or Median\n",
    "- Step 1: Load a dataset with missing values (e.g., Boston Housing dataset).\n",
    "- Step 2: Identify columns with missing values.\n",
    "- Step 3: Impute missing values using the mean or median of the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "# Introduce missing values artificially\n",
    "np.random.seed(42)\n",
    "df.loc[df.sample(frac=0.1).index, 'MedInc'] = np.nan\n",
    "\n",
    "missing_cols = df.columns[df.isnull().any()]\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[missing_cols] = imputer.fit_transform(df[missing_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Impute with the Most Frequent Value\n",
    "- Step 1: Use the Titanic dataset and identify columns with missing values.\n",
    "- Step 2: Impute categorical columns using the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "missing_cat_cols = [col for col in cat_cols if df[col].isnull().any()]\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[missing_cat_cols] = imputer.fit_transform(df[missing_cat_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Advanced Imputation - k-Nearest Neighbors\n",
    "- Step 1: Implement KNN imputation using the KNNImputer from sklearn.\n",
    "- Step 2: Explore how KNN imputation improves data completion over simpler methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df[num_cols] = imputer.fit_transform(df[num_cols])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & Normalization Best Practices:\n",
    "\n",
    "**Task 1**: Standardization\n",
    "- Step 1: Standardize features using StandardScaler.\n",
    "- Step 2: Observe how standardization affects data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = {'Age': [25, 32, 47, 51, 62], 'Income': [50000, 64000, 120000, 110000, 150000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Min-Max Scaling\n",
    "\n",
    "- Step 1: Scale features to lie between 0 and 1 using MinMaxScaler.\n",
    "- Step 2: Compare with standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = {'Age': [25, 32, 47, 51, 62], 'Income': [50000, 64000, 120000, 110000, 150000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_minmax_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Robust Scaling\n",
    "- Step 1: Scale features using RobustScaler, which is useful for data with outliers.\n",
    "- Step 2: Assess changes in data scaling compared to other scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "data = {'Age': [25, 32, 47, 51, 62, 150], 'Income': [50000, 64000, 120000, 110000, 150000, 1000000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "df_robust_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Techniques:\n",
    "### Removing Highly Correlated Features:\n",
    "\n",
    "**Task 1**: Correlation Matrix\n",
    "- Step 1: Compute correlation matrix.\n",
    "- Step 2: Remove highly correlated features (correlation > 0.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [2, 4, 6, 8, 10],\n",
    "    'C': [5, 3, 6, 2, 1],\n",
    "    'D': [1, 2, 2, 4, 5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "corr_matrix = df.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > 0.9)]\n",
    "df_reduced = df.drop(columns=to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Mutual Information & Variance Thresholds:\n",
    "\n",
    "**Task 2**: Mutual Information\n",
    "- Step 1: Compute mutual information between features and target.\n",
    "- Step 2: Retain features with high mutual information scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "X = pd.DataFrame({\n",
    "    \"feature1\": np.random.rand(100),\n",
    "    \"feature2\": np.random.rand(100),\n",
    "    \"feature3\": np.random.randint(0, 2, 100),\n",
    "    \"feature4\": np.random.rand(100)\n",
    "})\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "mi_scores = mutual_info_classif(X, y, discrete_features=[2])\n",
    "selected_features = X.columns[mi_scores > 0.05]\n",
    "selected_features.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Variance Threshold\n",
    "- Step 1: Implement VarianceThreshold to remove features with low variance.\n",
    "- Step 2: Analyze impact on feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "X = pd.DataFrame({\n",
    "    \"feature1\": np.random.rand(100),\n",
    "    \"feature2\": np.random.rand(100),\n",
    "    \"feature3\": np.random.randint(0, 2, 100),\n",
    "    \"feature4\": np.ones(100) * 0.5\n",
    "})\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_selected = selector.fit_transform(X)\n",
    "X_selected.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
